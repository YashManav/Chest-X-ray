{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25319,"status":"ok","timestamp":1759241240440,"user":{"displayName":"Yash Manav","userId":"01360529433997906882"},"user_tz":-330},"id":"269ca581","outputId":"bad64640-a822-425d-bee3-3b39b491219e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"WDisnARst6hX","outputId":"26d403ba-333a-4048-ea62-22020da15faa","executionInfo":{"status":"error","timestamp":1759245240586,"user_tz":-330,"elapsed":2184,"user":{"displayName":"Yash Manav","userId":"01360529433997906882"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5226 files belonging to 2 classes.\n","Found 16 files belonging to 2 classes.\n","Found 634 files belonging to 2 classes.\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'regularizers' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1606532555.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_baseline_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Baseline CNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;31m# results.extend(train_and_evaluate(build_dropout_cnn(), \"Dropout CNN\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;31m# results.extend(train_and_evaluate(build_batchnorm_cnn(), \"BatchNorm CNN\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1606532555.py\u001b[0m in \u001b[0;36mbuild_baseline_cnn\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     66\u001b[0m     model = models.Sequential([\n\u001b[1;32m     67\u001b[0m         layers.Conv2D(64, (3,3), activation='relu', padding='same', \n\u001b[0;32m---> 68\u001b[0;31m                      kernel_regularizer=regularizers.l2(0.001), input_shape=input_shape),\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         layers.Conv2D(64, (3,3), activation='relu', padding='same',\n","\u001b[0;31mNameError\u001b[0m: name 'regularizers' is not defined"]}],"source":["# ===============================\n","# Notebook 1 - Custom CNN Models\n","# ===============================\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","import numpy as np\n","import pandas as pd\n","import time\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import os\n","\n","# -----------------------\n","# Dataset Setup\n","# -----------------------\n","data_dir = \"/content/drive/MyDrive/PBL3/data/chest_xray\"\n","img_size = (224, 224)\n","batch_size = 32\n","\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","    os.path.join(data_dir, \"train\"),\n","    image_size=img_size,\n","    batch_size=batch_size\n",")\n","\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","    os.path.join(data_dir, \"val\"),\n","    image_size=img_size,\n","    batch_size=batch_size\n",")\n","\n","test_ds = tf.keras.utils.image_dataset_from_directory(\n","    os.path.join(data_dir, \"test\"),\n","    image_size=img_size,\n","    batch_size=batch_size,\n","    shuffle=False\n",")\n","\n","class_names = train_ds.class_names\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","train_ds = train_ds.cache().shuffle(500).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","# -----------------------\n","# Define CNN Models\n","# -----------------------\n","def build_baseline_cnn(input_shape=(224,224,3)):\n","    model = models.Sequential([\n","        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n","        layers.MaxPooling2D(2,2),\n","        layers.Conv2D(64, (3,3), activation='relu'),\n","        layers.MaxPooling2D(2,2),\n","        layers.Conv2D(128, (3,3), activation='relu'),\n","        layers.MaxPooling2D(2,2),\n","        layers.Flatten(),\n","        layers.Dense(128, activation='relu'),\n","        layers.Dense(1, activation='sigmoid')\n","\n","\n","    ])\n","    return model\n","\n","def build_dropout_cnn(input_shape=(224,224,3)):\n","    model = models.Sequential([\n","        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n","        layers.MaxPooling2D(2,2),\n","        layers.Dropout(0.25),\n","        layers.Conv2D(64, (3,3), activation='relu'),\n","        layers.MaxPooling2D(2,2),\n","        layers.Dropout(0.25),\n","        layers.Conv2D(128, (3,3), activation='relu'),\n","        layers.MaxPooling2D(2,2),\n","        layers.Flatten(),\n","        layers.Dense(128, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(1, activation='sigmoid')\n","    ])\n","    return model\n","\n","def build_batchnorm_cnn(input_shape=(224,224,3)):\n","    model = models.Sequential([\n","        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D(2,2),\n","        layers.Conv2D(64, (3,3), activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D(2,2),\n","        layers.Conv2D(128, (3,3), activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D(2,2),\n","        layers.Flatten(),\n","        layers.Dense(128, activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Dense(1, activation='sigmoid')\n","    ])\n","    return model\n","\n","def build_dropout_batchnorm_cnn(input_shape=(224,224,3)):\n","    model = models.Sequential([\n","        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D(2,2),\n","        layers.Dropout(0.25),\n","        layers.Conv2D(64, (3,3), activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D(2,2),\n","        layers.Dropout(0.25),\n","        layers.Conv2D(128, (3,3), activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D(2,2),\n","        layers.Flatten(),\n","        layers.Dense(128, activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Dropout(0.5),\n","        layers.Dense(1, activation='sigmoid')\n","    ])\n","    return model\n","\n","# -----------------------\n","# Custom Callback for Logging Test Metrics at Specific Epochs\n","# -----------------------\n","class EpochLogger(tf.keras.callbacks.Callback):\n","    def __init__(self, model_name, test_ds, results, epochs_to_log=[10,20,30,40,50]):\n","        super().__init__()\n","        self.model_name = model_name\n","        self.test_ds = test_ds\n","        self.results = results\n","        self.epochs_to_log = epochs_to_log\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        epoch_num = epoch + 1\n","        if epoch_num in self.epochs_to_log:\n","            y_true = np.concatenate([y.numpy() for x,y in self.test_ds], axis=0)\n","            y_prob = self.model.predict(self.test_ds, verbose=0)\n","            y_pred = (y_prob > 0.5).astype(\"int32\").flatten()\n","\n","            acc = accuracy_score(y_true, y_pred)\n","            prec = precision_score(y_true, y_pred, zero_division=0)\n","            rec = recall_score(y_true, y_pred, zero_division=0)\n","            f1 = f1_score(y_true, y_pred, zero_division=0)\n","            params = self.model.count_params()/1e6\n","\n","            self.results.append({\n","                \"Model\": self.model_name,\n","                \"Epoch\": epoch_num,\n","                \"Accuracy\": acc,\n","                \"F1-score\": f1,\n","                \"Precision\": prec,\n","                \"Recall\": rec,\n","                \"Params (M)\": params\n","            })\n","            print(f\"✅ Logged results at epoch {epoch_num}\")\n","\n","# -----------------------\n","# Training + Evaluation\n","# -----------------------\n","def train_and_evaluate(model, model_name, epochs=50):\n","    results = []\n","    model.compile(optimizer='adam',\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    start = time.time()\n","    model.fit(\n","        train_ds,\n","        validation_data=val_ds,\n","        epochs=epochs,\n","        verbose=1,\n","        callbacks=[EpochLogger(model_name, test_ds, results)]\n","    )\n","    end = time.time()\n","    train_time = (end-start)/60\n","\n","    for r in results:\n","        r[\"Train Time (mins)\"] = train_time\n","\n","    return results\n","\n","# -----------------------\n","# Run Experiments\n","# -----------------------\n","results = []\n","\n","results.extend(train_and_evaluate(build_baseline_cnn(), \"Baseline CNN\"))\n","# results.extend(train_and_evaluate(build_dropout_cnn(), \"Dropout CNN\"))\n","# results.extend(train_and_evaluate(build_batchnorm_cnn(), \"BatchNorm CNN\"))\n","# results.extend(train_and_evaluate(build_dropout_batchnorm_cnn(), \"Dropout+BatchNorm CNN\"))\n","\n","df = pd.DataFrame(results)\n","print(df)\n","\n","df.to_csv(\"/content/custom_results_summary.csv\", index=False)\n","print(\"✅ Saved to /content/custom_results_summary.csv\")"]},{"cell_type":"code","source":[],"metadata":{"id":"l_UFw83sMOZK"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNpAq2sZq1ZMJxbmJIiFxkA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}